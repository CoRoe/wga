{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1yauzULg1l2B6BL1tElNCfxEkgT5R9IWC",
      "authorship_tag": "ABX9TyPECxgXMSdaHkk+jDTFiz0X"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# About The Project\n",
        "\n",
        "Wayback Google Analytics is a lightweight tool that gathers current and historic Google analytics data (UA, GA and GTM codes) from a collection of website urls.\n",
        "\n",
        "Read Bellingcat's article about using this tool to uncover disinformation networks online [here](https://www.bellingcat.com/resources/2024/01/09/using-the-wayback-machine-and-google-analytics-to-uncover-disinformation-networks/).\n",
        "\n",
        "# Why do I need GA codes?\n",
        "\n",
        "Google Analytics codes are a useful data point when examining relationships between websites. If two seemingly disparate websites share the same UA, GA or GTM code then there is a good chance that they are managed by the same individual or group.\n",
        "\n",
        "This useful breadcrumb has been used by researchers and journalists in OSINT investigations regularly over the last decade, but a recent change in how Google handles its analytics codes threatens to limit its effectiveness. Google began phasing out UA codes as part of its Google Analytics 4 upgrade in July 2023, making it significantly more challenging to use this breadcrumb during investigations.\n",
        "\n",
        "# How does this notebook help me?\n",
        "\n",
        "Luckily, the Internet Archive's [Wayback Machine](https://archive.org/web/) contains useful snapshots of websites containing their historic GA IDs. While you could feasibly check each snapshot manually, this tool automates that process with the Wayback Machines CDX API to simplify and speed up the process. Enter a list of urls and a time frame (along with extra, optional parameters) to collect current and historic GA, UA and GTM codes.\n",
        "\n",
        "# Getting started\n",
        "\n",
        "Fill in the parameters in the form below and click the 'run' buttons. If all goes well, the notebook displays a diagram showing which UA/GA codes that were used by the websites."
      ],
      "metadata": {
        "id": "-Tu8R_1byC7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown #### Download some more code from Github:\n",
        "![ -d wga ] || git clone https://github.com/CoRoe/wga"
      ],
      "metadata": {
        "id": "22EHBFWXjJii"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown #### Enter one or more internet addresses separated by spaces to get their UA/GA codes:\n",
        "urls = \"cloudflare.net\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown #### Enter start and end dates:\n",
        "start_date = \"2014-05-29\" # @param {type:\"date\"}\n",
        "end_date = \"2014-06-30\" # @param {type:\"date\"}\n",
        "\n",
        "# @markdown #### Limits number of snapshots returned. Defaults to -100 (most recent snapshots)\n",
        "limit = -100 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown #### Can limit snapshots to remove duplicates (1 per hr, day, month, etc.)\n",
        "frequency = \"daily\" # @param [\"hourly\", \"daily\", \"monthly\", \"yearly\"]\n",
        "\n",
        "from datetime import datetime\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import sys\n",
        "from IPython.display import Image\n",
        "\n",
        "# Assuming Colab:\n",
        "if not '/content/wga' in sys.path:\n",
        "    sys.path.insert(0,'/content/wga')\n",
        "\n",
        "from wayback_google_analytics.utils import (\n",
        "    get_limit_from_frequency,\n",
        "    get_14_digit_timestamp,\n",
        "    validate_dates,\n",
        "    COLLAPSE_OPTIONS\n",
        ")\n",
        "\n",
        "from wayback_google_analytics.scraper import (\n",
        "    get_analytics_codes\n",
        ")\n",
        "\n",
        "from wayback_google_analytics.output import (\n",
        "    init_output,\n",
        "    write_output\n",
        ")\n",
        "\n",
        "\n",
        "async def main():\n",
        "    \"\"\"Main function. Runs get_analytics_codes() and prints results.\n",
        "\n",
        "    Args:\n",
        "        args: Command line arguments (argparse)\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    global urls, start_date, end_date, limit, frequency\n",
        "    print(\"Entered main\", start_date)\n",
        "\n",
        "    start_date = datetime.fromisoformat(start_date)\n",
        "    end_date = datetime.fromisoformat(end_date)\n",
        "\n",
        "    # Check if start_date is before end_date\n",
        "    if not start_date < end_date:\n",
        "        raise ValueError(\"Start date must be before end date.\")\n",
        "\n",
        "    # Update dates to 14-digit format\n",
        "    if start_date:\n",
        "        # start_date = get_14_digit_timestamp(start_date)\n",
        "        start_date = start_date.strftime(\"%Y%m%d%H%M%S\")\n",
        "\n",
        "    if end_date:\n",
        "        # end_date = get_14_digit_timestamp(end_date)\n",
        "        end_date = end_date.strftime(\"%Y%m%d%H%M%S\")\n",
        "\n",
        "    # Gets appropriate limit for given frequency & converts frequency to collapse option\n",
        "    if frequency:\n",
        "        limit = (\n",
        "            get_limit_from_frequency(\n",
        "                frequency=frequency,\n",
        "                start_date=start_date,\n",
        "                end_date=end_date,\n",
        "            )\n",
        "            + 1\n",
        "        )\n",
        "        frequency = COLLAPSE_OPTIONS[frequency]\n",
        "\n",
        "    semaphore = asyncio.Semaphore(10)\n",
        "\n",
        "    # Warn user if large request\n",
        "    if abs(int(limit)) > 500 or len(urls) > 9:\n",
        "        print(\"limit:\", limit)\n",
        "        response = input(\n",
        "            f\"\"\"Large requests can lead to being rate limited by archive.org.\\n\\n Current limit: {args.limit} (Recommended < 500) \\n\\n Current # of urls: {len(urls)} (Recommended < 10, unless limit < 50)\n",
        "\n",
        "        Do you wish to proceed? (Yes/no)\n",
        "                         \"\"\"\n",
        "        )\n",
        "        if response.lower() not in (\"yes\", \"y\"):\n",
        "            print(\"Request cancelled.\")\n",
        "            exit()\n",
        "\n",
        "    try:\n",
        "        async with semaphore:\n",
        "            async with aiohttp.ClientSession() as session:\n",
        "                results = await get_analytics_codes(\n",
        "                    session=session,\n",
        "                    urls=urls,\n",
        "                    start_date=start_date,\n",
        "                    end_date=end_date,\n",
        "                    frequency=frequency,\n",
        "                    limit=limit,\n",
        "                    semaphore=semaphore,\n",
        "                    skip_current=skip_current,\n",
        "                )\n",
        "                print(results)\n",
        "\n",
        "        write_output('output.dot', 'dot', results)\n",
        "    except aiohttp.ClientError as e:\n",
        "        print(\n",
        "            \"Your request was rate limited. Wait 5 minutes and try again and consider reducing the limit and # of numbers.\"\n",
        "        )\n",
        "\n",
        "\n",
        "#\n",
        "# Main\n",
        "#\n",
        "urls = urls.split()\n",
        "skip_current = False\n",
        "\n",
        "loop = asyncio.get_running_loop()\n",
        "await loop.create_task(main())\n",
        "\n",
        "# FIXME: In the original code, each run would create an output file with a\n",
        "# different name. This made it difficult to display it in the notebook.\n",
        "# Somehow try to return the name of the output file from the asynchronously\n",
        "# running main() function.\n",
        "Image('output.png')\n"
      ],
      "metadata": {
        "id": "Ud0PMhOc_8Cd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}